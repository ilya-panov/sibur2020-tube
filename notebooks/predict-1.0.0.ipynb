{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Что бы стали доступны модули из ./scripts\n",
    "sys.path.insert(0, os.path.abspath(\"../scripts\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from statsmodels.tsa.ar_model import AR\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "\n",
    "# Ignore harmless warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tube.tools.cleaner import Cleaner\n",
    "from tube.tools import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = \"/home/ilya/iWorkspace/research/sibur-2020/data/tube\"\n",
    "\n",
    "train_features = os.path.join(DATA, \"dataset\", \"train_features.csv\")\n",
    "train_targets = os.path.join(DATA, \"dataset\", \"train_targets.csv\")\n",
    "test_features = os.path.join(DATA, \"dataset\", \"test_features.csv\")\n",
    "\n",
    "MODELS = \"/home/ilya/iWorkspace/research/sibur-2020/data/tube/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = pd.read_csv(train_features, index_col=\"timestamp\", parse_dates=True)\n",
    "df_X.index.freq = \"0.5H\"\n",
    "\n",
    "df_Y = pd.read_csv(train_targets, index_col=\"timestamp\", parse_dates=True)\n",
    "df_Y.index.freq = \"0.5H\"\n",
    "\n",
    "df_test = pd.read_csv(test_features, index_col=\"timestamp\", parse_dates=True)\n",
    "df_test.index.freq = \"0.5H\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X, _ = dataset.clear_train_dataset(df_X, df_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 по полчаса - 1 час; 24 часа; 7 дней\n",
    "PREDICT_DEPTH = 2 * 24 * 7"
   ]
  },
  {
   "source": [
    "# RESULT"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## iC4H10"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "START [2020-05-01 00:00:00 ; 2020-05-07 23:30:00 )\n",
      "DONE [2020-05-01 00:00:00 ; 2020-05-07 23:30:00 )\n",
      "\n",
      "START [2020-05-08 00:00:00 ; 2020-05-14 23:30:00 )\n",
      "DONE [2020-05-08 00:00:00 ; 2020-05-14 23:30:00 )\n",
      "\n",
      "START [2020-05-15 00:00:00 ; 2020-05-21 23:30:00 )\n",
      "DONE [2020-05-15 00:00:00 ; 2020-05-21 23:30:00 )\n",
      "\n",
      "START [2020-05-22 00:00:00 ; 2020-05-28 23:30:00 )\n",
      "DONE [2020-05-22 00:00:00 ; 2020-05-28 23:30:00 )\n",
      "\n",
      "START [2020-05-29 00:00:00 ; 2020-06-04 23:30:00 )\n",
      "DONE [2020-05-29 00:00:00 ; 2020-06-04 23:30:00 )\n",
      "\n",
      "START [2020-06-05 00:00:00 ; 2020-06-11 23:30:00 )\n",
      "DONE [2020-06-05 00:00:00 ; 2020-06-11 23:30:00 )\n",
      "\n",
      "START [2020-06-12 00:00:00 ; 2020-06-18 23:30:00 )\n",
      "DONE [2020-06-12 00:00:00 ; 2020-06-18 23:30:00 )\n",
      "\n",
      "START [2020-06-19 00:00:00 ; 2020-06-25 23:30:00 )\n",
      "DONE [2020-06-19 00:00:00 ; 2020-06-25 23:30:00 )\n",
      "\n",
      "START [2020-06-26 00:00:00 ; 2020-07-02 23:30:00 )\n",
      "DONE [2020-06-26 00:00:00 ; 2020-07-02 23:30:00 )\n",
      "\n",
      "START [2020-07-03 00:00:00 ; 2020-07-09 23:30:00 )\n",
      "DONE [2020-07-03 00:00:00 ; 2020-07-09 23:30:00 )\n",
      "\n",
      "START [2020-07-10 00:00:00 ; 2020-07-16 23:30:00 )\n",
      "DONE [2020-07-10 00:00:00 ; 2020-07-16 23:30:00 )\n",
      "\n",
      "START [2020-07-17 00:00:00 ; 2020-07-22 23:30:00 )\n",
      "DONE [2020-07-17 00:00:00 ; 2020-07-22 23:30:00 )\n",
      "\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "NAME = \"iC4H10\"\n",
    "\n",
    "RESULT = []\n",
    "\n",
    "FORECAST_DEPTH = PREDICT_DEPTH + 300 + 1\n",
    "\n",
    "df = df_X.copy()\n",
    "df_for_predict = None\n",
    "\n",
    "first = True\n",
    "start = 0\n",
    "end = PREDICT_DEPTH\n",
    "\n",
    "cleaner = Cleaner()\n",
    "\n",
    "length = len(df_test)\n",
    "\n",
    "fpath = os.path.join(MODELS, NAME + \".lr\")\n",
    "with open(fpath, 'rb') as fp:\n",
    "    lr = pickle.load(fp)\n",
    "\n",
    "while True:\n",
    "\n",
    "    #print(start, end)\n",
    "\n",
    "    if start > length:\n",
    "        break\n",
    "\n",
    "    known = df_test.iloc[ start : end ].copy()\n",
    "\n",
    "    start_date = known.index[0]\n",
    "    end_date = known.index[-1]\n",
    "    print(\"START [{0} ; {1} )\". format(start_date, end_date))\n",
    "\n",
    "    ext_indx = pd.date_range(\n",
    "        start=start_date,\n",
    "        freq=\"0.5H\",\n",
    "        periods=FORECAST_DEPTH)\n",
    "    df_for_predict = pd.DataFrame(\n",
    "        np.zeros( (FORECAST_DEPTH, len(df.columns)) ),\n",
    "        index=ext_indx,\n",
    "        columns=df.columns)\n",
    "    df_for_predict.index.name = 'timestamp'\n",
    "\n",
    "    # FORECAST\n",
    "\n",
    "    model = AR(df[ 'A_' + NAME ])\n",
    "    ARfit = model.fit(method='mle')\n",
    "\n",
    "    forecast_start = len(df)\n",
    "    forecast_end = forecast_start + FORECAST_DEPTH\n",
    "    rename = 'AR(11)'\n",
    "\n",
    "    fcasts = ARfit.predict(start=forecast_start, end=forecast_end, dynamic=False).rename(rename)\n",
    "\n",
    "    # Замена известных значений (они еще в будущем) на предсказанные\n",
    "    i = 0\n",
    "    for _, row in df_for_predict.iterrows():\n",
    "        row[ 'A_' + NAME ] = fcasts.iloc[i]\n",
    "        i += 1\n",
    "    \n",
    "    df_X_for_predict = pd.concat([df, df_for_predict])\n",
    "\n",
    "\n",
    "    #PREDICT\n",
    "\n",
    "    ds = dataset.prepare_eval_dataset(NAME, df_X_for_predict.shift(5))\n",
    "\n",
    "    A_val = \"A_{0}\".format(NAME)\n",
    "    B_val = \"B_{0}\".format(NAME)\n",
    "\n",
    "    for column in [A_val]:\n",
    "        for i in range(1, 300):\n",
    "            ds[ \"{0}-{1}\".format(column, i) ] = ds[ column ].shift(i)\n",
    "            ds[ \"{0}+{1}\".format(column, i) ] = ds[ column ].shift(-i)\n",
    "    \n",
    "    X = ds.dropna().drop([\"A_rate\", \"B_rate\"], axis=1)\n",
    "    X = X[ start_date : end_date]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    predictions = lr.predict(X_scaled)\n",
    "\n",
    "    # RESULT\n",
    "\n",
    "    new_result = pd.DataFrame(predictions,\n",
    "        index=X.index,\n",
    "        columns=['B_' + NAME])\n",
    "    new_result.index.name = 'timestamp'\n",
    "\n",
    "    RESULT.append(new_result)\n",
    "\n",
    "\n",
    "    df = pd.concat([df, known])\n",
    "    df = cleaner.clean(df)\n",
    "\n",
    "    start += PREDICT_DEPTH\n",
    "    end += PREDICT_DEPTH\n",
    "\n",
    "    #print(\"[{0} ; {1} )\". format(df.index[0], df.index[-1]))\n",
    "\n",
    "    print(\"DONE [{0} ; {1} )\". format(start_date, end_date))\n",
    "    print()\n",
    "    #break\n",
    "\n",
    "print(\"DONE\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "iC4H10 = pd.concat(RESULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE\n",
    "iC4H10.to_csv(\"./submission.csv\")"
   ]
  },
  {
   "source": [
    "## C2H6"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "START [2020-05-01 00:00:00 ; 2020-05-07 23:30:00 )\n",
      "DONE [2020-05-01 00:00:00 ; 2020-05-07 23:30:00 )\n",
      "\n",
      "START [2020-05-08 00:00:00 ; 2020-05-14 23:30:00 )\n",
      "DONE [2020-05-08 00:00:00 ; 2020-05-14 23:30:00 )\n",
      "\n",
      "START [2020-05-15 00:00:00 ; 2020-05-21 23:30:00 )\n",
      "DONE [2020-05-15 00:00:00 ; 2020-05-21 23:30:00 )\n",
      "\n",
      "START [2020-05-22 00:00:00 ; 2020-05-28 23:30:00 )\n",
      "DONE [2020-05-22 00:00:00 ; 2020-05-28 23:30:00 )\n",
      "\n",
      "START [2020-05-29 00:00:00 ; 2020-06-04 23:30:00 )\n",
      "DONE [2020-05-29 00:00:00 ; 2020-06-04 23:30:00 )\n",
      "\n",
      "START [2020-06-05 00:00:00 ; 2020-06-11 23:30:00 )\n",
      "DONE [2020-06-05 00:00:00 ; 2020-06-11 23:30:00 )\n",
      "\n",
      "START [2020-06-12 00:00:00 ; 2020-06-18 23:30:00 )\n",
      "DONE [2020-06-12 00:00:00 ; 2020-06-18 23:30:00 )\n",
      "\n",
      "START [2020-06-19 00:00:00 ; 2020-06-25 23:30:00 )\n",
      "DONE [2020-06-19 00:00:00 ; 2020-06-25 23:30:00 )\n",
      "\n",
      "START [2020-06-26 00:00:00 ; 2020-07-02 23:30:00 )\n",
      "DONE [2020-06-26 00:00:00 ; 2020-07-02 23:30:00 )\n",
      "\n",
      "START [2020-07-03 00:00:00 ; 2020-07-09 23:30:00 )\n",
      "DONE [2020-07-03 00:00:00 ; 2020-07-09 23:30:00 )\n",
      "\n",
      "START [2020-07-10 00:00:00 ; 2020-07-16 23:30:00 )\n",
      "DONE [2020-07-10 00:00:00 ; 2020-07-16 23:30:00 )\n",
      "\n",
      "START [2020-07-17 00:00:00 ; 2020-07-22 23:30:00 )\n",
      "DONE [2020-07-17 00:00:00 ; 2020-07-22 23:30:00 )\n",
      "\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "NAME = \"C2H6\"\n",
    "\n",
    "RESULT = []\n",
    "\n",
    "FORECAST_DEPTH = PREDICT_DEPTH + 300 + 1\n",
    "\n",
    "df = df_X.copy()\n",
    "df_for_predict = None\n",
    "\n",
    "first = True\n",
    "start = 0\n",
    "end = PREDICT_DEPTH\n",
    "\n",
    "cleaner = Cleaner()\n",
    "\n",
    "length = len(df_test)\n",
    "\n",
    "fpath = os.path.join(MODELS, NAME + \".lr\")\n",
    "with open(fpath, 'rb') as fp:\n",
    "    lr = pickle.load(fp)\n",
    "\n",
    "while True:\n",
    "\n",
    "    #print(start, end)\n",
    "\n",
    "    if start > length:\n",
    "        break\n",
    "\n",
    "    known = df_test.iloc[ start : end ].copy()\n",
    "\n",
    "    start_date = known.index[0]\n",
    "    end_date = known.index[-1]\n",
    "    print(\"START [{0} ; {1} )\". format(start_date, end_date))\n",
    "\n",
    "    ext_indx = pd.date_range(\n",
    "        start=start_date,\n",
    "        freq=\"0.5H\",\n",
    "        periods=FORECAST_DEPTH)\n",
    "    df_for_predict = pd.DataFrame(\n",
    "        np.zeros( (FORECAST_DEPTH, len(df.columns)) ),\n",
    "        index=ext_indx,\n",
    "        columns=df.columns)\n",
    "    df_for_predict.index.name = 'timestamp'\n",
    "\n",
    "    # FORECAST\n",
    "\n",
    "    model = AR(df[ 'A_' + NAME ])\n",
    "    ARfit = model.fit(method='mle')\n",
    "\n",
    "    forecast_start = len(df)\n",
    "    forecast_end = forecast_start + FORECAST_DEPTH\n",
    "    rename = 'AR(11)'\n",
    "\n",
    "    fcasts = ARfit.predict(start=forecast_start, end=forecast_end, dynamic=False).rename(rename)\n",
    "\n",
    "    # Замена известных значений (они еще в будущем) на предсказанные\n",
    "    i = 0\n",
    "    for _, row in df_for_predict.iterrows():\n",
    "        row[ 'A_' + NAME ] = fcasts.iloc[i]\n",
    "        i += 1\n",
    "    \n",
    "\n",
    "    model = AR(df[ 'A_rate' ])\n",
    "    ARfit = model.fit(method='mle')\n",
    "\n",
    "    fcasts = ARfit.predict(start=forecast_start, end=forecast_end, dynamic=False).rename(rename)\n",
    "\n",
    "    # Замена известных значений (они еще в будущем) на предсказанные\n",
    "    i = 0\n",
    "    for _, row in df_for_predict.iterrows():\n",
    "        row[ 'A_rate' ] = fcasts.iloc[i]\n",
    "        i += 1\n",
    "\n",
    "\n",
    "    model = AR(df[ 'B_rate' ])\n",
    "    ARfit = model.fit(method='mle')\n",
    "\n",
    "    fcasts = ARfit.predict(start=forecast_start, end=forecast_end, dynamic=False).rename(rename)\n",
    "\n",
    "    # Замена известных значений (они еще в будущем) на предсказанные\n",
    "    i = 0\n",
    "    for _, row in df_for_predict.iterrows():\n",
    "        row[ 'B_rate' ] = fcasts.iloc[i]\n",
    "        i += 1\n",
    "\n",
    "    df_X_for_predict = pd.concat([df, df_for_predict])\n",
    "\n",
    "\n",
    "    #PREDICT\n",
    "\n",
    "    ds = dataset.prepare_eval_dataset(NAME, df_X_for_predict.shift(10))\n",
    "\n",
    "    A_val = \"A_{0}\".format(NAME)\n",
    "    B_val = \"B_{0}\".format(NAME)\n",
    "\n",
    "    for column in [A_val]:\n",
    "        for i in range(170, 300):\n",
    "            ds[ \"{0}-{1}\".format(column, i) ] = ds[ column ].shift(i)\n",
    "            ds[ \"{0}+{1}\".format(column, i) ] = ds[ column ].shift(-i)\n",
    "    \n",
    "    X = ds.dropna()\n",
    "    X = X[ start_date : end_date]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    predictions = lr.predict(X_scaled)\n",
    "\n",
    "    # RESULT\n",
    "\n",
    "    new_result = pd.DataFrame(predictions,\n",
    "        index=X.index,\n",
    "        columns=['B_' + NAME])\n",
    "    new_result.index.name = 'timestamp'\n",
    "\n",
    "    RESULT.append(new_result)\n",
    "\n",
    "\n",
    "    df = pd.concat([df, known])\n",
    "    df = cleaner.clean(df)\n",
    "\n",
    "    start += PREDICT_DEPTH\n",
    "    end += PREDICT_DEPTH\n",
    "\n",
    "    #print(\"[{0} ; {1} )\". format(df.index[0], df.index[-1]))\n",
    "\n",
    "    print(\"DONE [{0} ; {1} )\". format(start_date, end_date))\n",
    "    print()\n",
    "    #break\n",
    "\n",
    "print(\"DONE\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "C2H6 = pd.concat(RESULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE\n",
    "C2H6.to_csv(\"./submission.csv\")"
   ]
  },
  {
   "source": [
    "## C3H8"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "START [2020-05-01 00:00:00 ; 2020-05-07 23:30:00 )\n",
      "DONE [2020-05-01 00:00:00 ; 2020-05-07 23:30:00 )\n",
      "\n",
      "START [2020-05-08 00:00:00 ; 2020-05-14 23:30:00 )\n",
      "DONE [2020-05-08 00:00:00 ; 2020-05-14 23:30:00 )\n",
      "\n",
      "START [2020-05-15 00:00:00 ; 2020-05-21 23:30:00 )\n",
      "DONE [2020-05-15 00:00:00 ; 2020-05-21 23:30:00 )\n",
      "\n",
      "START [2020-05-22 00:00:00 ; 2020-05-28 23:30:00 )\n",
      "DONE [2020-05-22 00:00:00 ; 2020-05-28 23:30:00 )\n",
      "\n",
      "START [2020-05-29 00:00:00 ; 2020-06-04 23:30:00 )\n",
      "DONE [2020-05-29 00:00:00 ; 2020-06-04 23:30:00 )\n",
      "\n",
      "START [2020-06-05 00:00:00 ; 2020-06-11 23:30:00 )\n",
      "DONE [2020-06-05 00:00:00 ; 2020-06-11 23:30:00 )\n",
      "\n",
      "START [2020-06-12 00:00:00 ; 2020-06-18 23:30:00 )\n",
      "DONE [2020-06-12 00:00:00 ; 2020-06-18 23:30:00 )\n",
      "\n",
      "START [2020-06-19 00:00:00 ; 2020-06-25 23:30:00 )\n",
      "DONE [2020-06-19 00:00:00 ; 2020-06-25 23:30:00 )\n",
      "\n",
      "START [2020-06-26 00:00:00 ; 2020-07-02 23:30:00 )\n",
      "DONE [2020-06-26 00:00:00 ; 2020-07-02 23:30:00 )\n",
      "\n",
      "START [2020-07-03 00:00:00 ; 2020-07-09 23:30:00 )\n",
      "DONE [2020-07-03 00:00:00 ; 2020-07-09 23:30:00 )\n",
      "\n",
      "START [2020-07-10 00:00:00 ; 2020-07-16 23:30:00 )\n",
      "DONE [2020-07-10 00:00:00 ; 2020-07-16 23:30:00 )\n",
      "\n",
      "START [2020-07-17 00:00:00 ; 2020-07-22 23:30:00 )\n",
      "DONE [2020-07-17 00:00:00 ; 2020-07-22 23:30:00 )\n",
      "\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "NAME = \"C3H8\"\n",
    "\n",
    "RESULT = []\n",
    "\n",
    "FORECAST_DEPTH = PREDICT_DEPTH + 20\n",
    "\n",
    "df = df_X.copy()\n",
    "df_for_predict = None\n",
    "\n",
    "first = True\n",
    "start = 0\n",
    "end = PREDICT_DEPTH\n",
    "\n",
    "cleaner = Cleaner()\n",
    "\n",
    "length = len(df_test)\n",
    "\n",
    "fpath = os.path.join(MODELS, NAME + \".lr\")\n",
    "with open(fpath, 'rb') as fp:\n",
    "    lr = pickle.load(fp)\n",
    "\n",
    "while True:\n",
    "\n",
    "    #print(start, end)\n",
    "\n",
    "    if start > length:\n",
    "        break\n",
    "\n",
    "    known = df_test.iloc[ start : end ].copy()\n",
    "\n",
    "    start_date = known.index[0]\n",
    "    end_date = known.index[-1]\n",
    "    print(\"START [{0} ; {1} )\". format(start_date, end_date))\n",
    "\n",
    "    ext_indx = pd.date_range(\n",
    "        start=start_date,\n",
    "        freq=\"0.5H\",\n",
    "        periods=FORECAST_DEPTH)\n",
    "    df_for_predict = pd.DataFrame(\n",
    "        np.zeros( (FORECAST_DEPTH, len(df.columns)) ),\n",
    "        index=ext_indx,\n",
    "        columns=df.columns)\n",
    "    df_for_predict.index.name = 'timestamp'\n",
    "\n",
    "    # FORECAST\n",
    "\n",
    "    model = AR(df[ 'A_' + NAME ])\n",
    "    ARfit = model.fit(method='mle')\n",
    "\n",
    "    forecast_start = len(df)\n",
    "    forecast_end = forecast_start + FORECAST_DEPTH\n",
    "    rename = 'AR(11)'\n",
    "\n",
    "    fcasts = ARfit.predict(start=forecast_start, end=forecast_end, dynamic=False).rename(rename)\n",
    "\n",
    "    # Замена известных значений (они еще в будущем) на предсказанные\n",
    "    i = 0\n",
    "    for _, row in df_for_predict.iterrows():\n",
    "        row[ 'A_' + NAME ] = fcasts.iloc[i]\n",
    "        i += 1\n",
    "\n",
    "\n",
    "    df_X_for_predict = pd.concat([df, df_for_predict])\n",
    "\n",
    "\n",
    "    #PREDICT\n",
    "\n",
    "    ds = dataset.prepare_eval_dataset(NAME, df_X_for_predict.shift(55))\n",
    "\n",
    "    A_val = \"A_{0}\".format(NAME)\n",
    "    B_val = \"B_{0}\".format(NAME)\n",
    "\n",
    "    for column in [A_val]:\n",
    "        for i in range(45, 100):\n",
    "            ds[ \"{0}-{1}\".format(column, i) ] = ds[ column ].shift(i)\n",
    "            #ds[ \"{0}+{1}\".format(column, i) ] = ds[ column ].shift(-i)\n",
    "    \n",
    "    X = ds.dropna().drop([\"A_rate\", \"B_rate\"], axis=1)\n",
    "    X = X[ start_date : end_date]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    predictions = lr.predict(X_scaled)\n",
    "\n",
    "    # RESULT\n",
    "\n",
    "    new_result = pd.DataFrame(predictions,\n",
    "        index=X.index,\n",
    "        columns=['B_' + NAME])\n",
    "    new_result.index.name = 'timestamp'\n",
    "\n",
    "    RESULT.append(new_result)\n",
    "\n",
    "\n",
    "    df = pd.concat([df, known])\n",
    "    df = cleaner.clean(df)\n",
    "\n",
    "    start += PREDICT_DEPTH\n",
    "    end += PREDICT_DEPTH\n",
    "\n",
    "    #print(\"[{0} ; {1} )\". format(df.index[0], df.index[-1]))\n",
    "\n",
    "    print(\"DONE [{0} ; {1} )\". format(start_date, end_date))\n",
    "    print()\n",
    "    #break\n",
    "\n",
    "print(\"DONE\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "C3H8 = pd.concat(RESULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE\n",
    "C3H8.to_csv(\"./submission.csv\")"
   ]
  },
  {
   "source": [
    "## nC4H10"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "START [2020-05-01 00:00:00 ; 2020-05-07 23:30:00 )\n",
      "DONE [2020-05-01 00:00:00 ; 2020-05-07 23:30:00 )\n",
      "\n",
      "START [2020-05-08 00:00:00 ; 2020-05-14 23:30:00 )\n",
      "DONE [2020-05-08 00:00:00 ; 2020-05-14 23:30:00 )\n",
      "\n",
      "START [2020-05-15 00:00:00 ; 2020-05-21 23:30:00 )\n",
      "DONE [2020-05-15 00:00:00 ; 2020-05-21 23:30:00 )\n",
      "\n",
      "START [2020-05-22 00:00:00 ; 2020-05-28 23:30:00 )\n",
      "DONE [2020-05-22 00:00:00 ; 2020-05-28 23:30:00 )\n",
      "\n",
      "START [2020-05-29 00:00:00 ; 2020-06-04 23:30:00 )\n",
      "DONE [2020-05-29 00:00:00 ; 2020-06-04 23:30:00 )\n",
      "\n",
      "START [2020-06-05 00:00:00 ; 2020-06-11 23:30:00 )\n",
      "DONE [2020-06-05 00:00:00 ; 2020-06-11 23:30:00 )\n",
      "\n",
      "START [2020-06-12 00:00:00 ; 2020-06-18 23:30:00 )\n",
      "DONE [2020-06-12 00:00:00 ; 2020-06-18 23:30:00 )\n",
      "\n",
      "START [2020-06-19 00:00:00 ; 2020-06-25 23:30:00 )\n",
      "DONE [2020-06-19 00:00:00 ; 2020-06-25 23:30:00 )\n",
      "\n",
      "START [2020-06-26 00:00:00 ; 2020-07-02 23:30:00 )\n",
      "DONE [2020-06-26 00:00:00 ; 2020-07-02 23:30:00 )\n",
      "\n",
      "START [2020-07-03 00:00:00 ; 2020-07-09 23:30:00 )\n",
      "DONE [2020-07-03 00:00:00 ; 2020-07-09 23:30:00 )\n",
      "\n",
      "START [2020-07-10 00:00:00 ; 2020-07-16 23:30:00 )\n",
      "DONE [2020-07-10 00:00:00 ; 2020-07-16 23:30:00 )\n",
      "\n",
      "START [2020-07-17 00:00:00 ; 2020-07-22 23:30:00 )\n",
      "DONE [2020-07-17 00:00:00 ; 2020-07-22 23:30:00 )\n",
      "\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "NAME = \"nC4H10\"\n",
    "\n",
    "RESULT = []\n",
    "\n",
    "FORECAST_DEPTH = PREDICT_DEPTH + 350 + 1\n",
    "\n",
    "df = df_X.copy()\n",
    "df_for_predict = None\n",
    "\n",
    "first = True\n",
    "start = 0\n",
    "end = PREDICT_DEPTH\n",
    "\n",
    "cleaner = Cleaner()\n",
    "\n",
    "length = len(df_test)\n",
    "\n",
    "fpath = os.path.join(MODELS, NAME + \".lr\")\n",
    "with open(fpath, 'rb') as fp:\n",
    "    lr = pickle.load(fp)\n",
    "\n",
    "while True:\n",
    "\n",
    "    #print(start, end)\n",
    "\n",
    "    if start > length:\n",
    "        break\n",
    "\n",
    "    known = df_test.iloc[ start : end ].copy()\n",
    "\n",
    "    start_date = known.index[0]\n",
    "    end_date = known.index[-1]\n",
    "    print(\"START [{0} ; {1} )\". format(start_date, end_date))\n",
    "\n",
    "    ext_indx = pd.date_range(\n",
    "        start=start_date,\n",
    "        freq=\"0.5H\",\n",
    "        periods=FORECAST_DEPTH)\n",
    "    df_for_predict = pd.DataFrame(\n",
    "        np.zeros( (FORECAST_DEPTH, len(df.columns)) ),\n",
    "        index=ext_indx,\n",
    "        columns=df.columns)\n",
    "    df_for_predict.index.name = 'timestamp'\n",
    "\n",
    "    # FORECAST\n",
    "\n",
    "    model = AR(df[ 'A_' + NAME ])\n",
    "    ARfit = model.fit(method='mle')\n",
    "\n",
    "    forecast_start = len(df)\n",
    "    forecast_end = forecast_start + FORECAST_DEPTH\n",
    "    rename = 'AR(11)'\n",
    "\n",
    "    fcasts = ARfit.predict(start=forecast_start, end=forecast_end, dynamic=False).rename(rename)\n",
    "\n",
    "    # Замена известных значений (они еще в будущем) на предсказанные\n",
    "    i = 0\n",
    "    for _, row in df_for_predict.iterrows():\n",
    "        row[ 'A_' + NAME ] = fcasts.iloc[i]\n",
    "        i += 1\n",
    "    \n",
    "    df_X_for_predict = pd.concat([df, df_for_predict])\n",
    "\n",
    "\n",
    "    #PREDICT\n",
    "\n",
    "    ds = dataset.prepare_eval_dataset(NAME, df_X_for_predict)\n",
    "\n",
    "    A_val = \"A_{0}\".format(NAME)\n",
    "    B_val = \"B_{0}\".format(NAME)\n",
    "\n",
    "    for column in [A_val]:\n",
    "        for i in range(1, 350):\n",
    "            ds[ \"{0}-{1}\".format(column, i) ] = ds[ column ].shift(i)\n",
    "            ds[ \"{0}+{1}\".format(column, i) ] = ds[ column ].shift(-i)\n",
    "    \n",
    "    X = ds.dropna().drop([\"A_rate\", \"B_rate\"], axis=1)\n",
    "    X = X[ start_date : end_date]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    predictions = lr.predict(X_scaled)\n",
    "\n",
    "    # RESULT\n",
    "\n",
    "    new_result = pd.DataFrame(predictions,\n",
    "        index=X.index,\n",
    "        columns=['B_' + NAME])\n",
    "    new_result.index.name = 'timestamp'\n",
    "\n",
    "    RESULT.append(new_result)\n",
    "\n",
    "\n",
    "    df = pd.concat([df, known])\n",
    "    df = cleaner.clean(df)\n",
    "\n",
    "    start += PREDICT_DEPTH\n",
    "    end += PREDICT_DEPTH\n",
    "\n",
    "    #print(\"[{0} ; {1} )\". format(df.index[0], df.index[-1]))\n",
    "\n",
    "    print(\"DONE [{0} ; {1} )\". format(start_date, end_date))\n",
    "    print()\n",
    "    #break\n",
    "\n",
    "print(\"DONE\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "nC4H10 = pd.concat(RESULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE\n",
    "nC4H10.to_csv(\"./submission.csv\")"
   ]
  },
  {
   "source": [
    "# RESULT"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = [iC4H10, C2H6, C3H8, nC4H10]\n",
    "result = pd.concat(all, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(\"./result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}